org.apache.curator:
NodeCache(先判断是否exist,在调用getData)
org.apache.curator.framework.recipes.cache.NodeCache.start
org.apache.curator.framework.recipes.cache.NodeCache.reset
org.apache.curator.framework.imps.ExistsBuilderImpl.forPath
org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation
org.apache.curator.framework.imps.ExistsBuilderImpl.performBackgroundOperation
org.apache.zookeeper.ZooKeeper.exists

org.apache.curator.framework.recipes.cache.NodeCache.processBackgroundResult
org.apache.curator.framework.imps.GetDataBuilderImpl.forPath

PathChildrenCache

TreeCache(NodeCache + PathChildrenCache)

CuratorFramework start:
org.apache.curator.framework.imps.CuratorFrameworkImpl.start
org.apache.curator.CuratorZookeeperClient.start
org.apache.curator.ConnectionState.start
org.apache.curator.ConnectionState.reset
org.apache.curator.HandleHolder.closeAndReset
org.apache.curator.HandleHolder.getZooKeeper

org.apache.zookeeper:
Leader Election -> Recovery -> Broadcast
watcher triggered:(watcher注册只是往server设置node上watch的boolean值)(单线程)
org.apache.zookeeper.ClientCnxn.EventThread.processEvent
send packet:(单线程,单连接)
org.apache.zookeeper.ClientCnxn.SendThread.sendPacket
应用层多线程写入outgoingQueue，SendThread取出outgoingQueue后写入到pendingQueue和发送到server(server按接收顺序处理请求)，SendThread接收到响应取出pendingQueue并返回给应用层
callback:
客户端命令调用完毕后的回调
client入口: org.apache.zookeeper.ZooKeeper server入口: org.apache.zookeeper.server.quorum.QuorumPeerMain
io：ClientCnxn ServerCnxn QuorumCnxManager(election)

session timeout -> con lost

Paxos到Zookeeper: 2 4 6 7
4. zab协议和paxos协议
6. zookeeper的应用场景
7. zookeeper技术内幕
-- watch通知实现
-- 会话创建,管理,清理
-- 选举算法细节
-- 请求处理

连接:
zookeeper: 长连接 集群里的单机
dia: http 随机长轮询 集群里的单机
config: 长连接 集群里的单机

PERSISTENT 持久型
创建了就会一直存在，直到被手动调用删除节点方法。
PERSISTENT_SEQUENTIAL 持久顺序型
会自动在节点路径名称后面添加一个自增的序号，如apple/iphone/macbook00000001，这种带自增序号的节点能保证输入同一路径时都能创建一个唯一的节点，可用于实现分布式队列，分布式公平锁。
EPHEMERAL 临时型
在回话结束后，节点自动删除。可用于健康检查。
EPHEMERAL_SEQUENTIAL 临时顺序型
临时节点特性，路径自动添加自增序号。
PERSISTENT_WITH_TTL
PERSISTENT_SEQUENTIAL_WITH_TTL
当该节点下面没有子节点的话，超过了 TTL 指定时间后就会被自动删除

zookeeper=文件系统+通知机制
使用的场景: 1.数据发布/订阅 2.master选举(集群管理) 3.命名服务(注册中心) 4. 复杂均衡 5. 分布式锁 6. 分布式队列 7. 配置管理
事务编号ZXID，是一个64位的数字: 前32位存储Leader周期届数epoch，后32位记录本届Leader处理的消息数
ZAB协议(zookeeper atomic broadcast 原子消息广播): Leader Election -> Recovery -> Broadcase
广播模式:
2PC 协议，所有事务请求都由Leader服务器来处理分发(非Leader服务器会转发事务请求)，Leader负责将请求封装成Proposal分发给集群中所有Follower，
一旦收到超过半数的正确反馈，Leader就会再次向所有的Follower分发Commit消息，要求他们将前一个Prosocal提交
崩溃恢复模式:(一旦Leader服务器出现崩溃或丢失一半Follower服务器的联系)
丢弃只在Leader服务器上未commit的事务，重新选举出来的Leader拥有集群中所有服务器最高的ZXID，保证在Leader服务器上已经commit的事务继续被其他服务器提交
Leader Election:
Leader: 1.事物请求的唯一调度和处理者，保证集群事务处理顺序性，2.集群内部各服务器调度者
Follower: 1.处理客户端非事物请求，转发事物请求给Leader服务器 2.参与事物请求Proposal的投票 3.参与Leader选举投票
Observer: 不参与任何形式的投票，只提供非事务处理服务。
Server(Observer除外)有三种状态：
LOOKING：当前Server不知道leader是谁，正在搜寻
LEADING：当前Server即为选举出来的leader
FOLLOWING：leader已经选举出来，当前Server与之同步
fast paxos算法: (还有basic paxos)
每个server发出一个投票。初始化阶段都会投给自己。投票以（myid，ZXID）表示。群发给所有server。
接收来自各个服务器的投票。判断投票的有效性、检查是否是本轮投票、是否来自looking状态的服务器
处理投票。把自己的投票和其他服务器的投票进行PK。PK规则：ZXID比较大的优先成为leader；ZXID相同，myid比较大的优先。把pk结果重新发给其他服务器
统计投票。每次投票后，都会统计是否有超过半数机器接收到相同的投票，大于等于n/2+1。如果没有结果则重新发起投票。
Leader选举结束后更改server状态，进行数据同步，向外提供服务。

羊群效应(带头羊一旦行动起来，羊群里的其他所有羊都进行响应)
当一个节点删除时，所有监听此节点的的客户端都会被通知然后进行节点创建，但最终只有一个客户端可以成功，结果造成网络负载和zookeeper性能负担(分布式锁时常常发生，使用顺序节点同时只监听当前节点的前一个节点，也就是公平锁)

脑裂
投票大于等于n/2+1，即quorum